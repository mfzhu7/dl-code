{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "## 本文件实现了基础版本的循环神经网络，参考代码来源于李沐老师\n",
    "## 数据集也参考来自李沐老师的代码\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "class RNNModelScratch:\n",
    "    def __init__(self, batch_size, vocab_size, num_hiddens, device_name):\n",
    "        self.vocab_size = vocab_size \n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.device = device_name\n",
    "        self.params  = self.params(vocab_size, num_hiddens, self.device)\n",
    "        self.state = torch.zeros((batch_size, num_hiddens), device=device_name) \n",
    "\n",
    "    def __call__(self, inputs):\n",
    "         inputs = F.one_hot(inputs.T).type(torch.float32)\n",
    "         inputs = inputs.to(self.device)\n",
    "         W_xh, W_hh, b_h, W_hq, b_q = self.params\n",
    "         H = self.state\n",
    "         outputs = []\n",
    "         for X in inputs:\n",
    "            H = torch.tanh(torch.mm(X,W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "            Y = torch.mm(H,W_hq) + b_q\n",
    "            outputs.append(Y)\n",
    "         return torch.cat(outputs, dim=0), (H,)\n",
    "\n",
    "\n",
    "    def normal(self, shape):\n",
    "        return torch.randn(size=shape, device=self.device)\n",
    "\n",
    "    def params(self, vocab_size, num_hiddens, device):\n",
    "        num_inputs = num_outputs = vocab_size\n",
    "\n",
    "        W_xh = self.normal((num_inputs, num_hiddens))\n",
    "        W_hh = self.normal((num_hiddens,num_hiddens))\n",
    "        b_h = torch.zeros(num_hiddens,device=self.device)\n",
    "\n",
    "        W_hq = self.normal((num_hiddens, num_outputs))\n",
    "        b_q = torch.zeros(num_outputs, device=self.device)\n",
    "\n",
    "        params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "\n",
    "        for param in params:\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        return params \n",
    "\n",
    "\n",
    "\n",
    "def train_one_step(net, train_iter, loss, updater):\n",
    "    for X, Y in train_iter:\n",
    "        l = loss(net(X)[0]).mean()\n",
    "        updater.zero_grad()\n",
    "        l.backward() \n",
    "        updater.step()\n",
    "\n",
    "\n",
    "# def train_rnn(net,train_iter, epoch, lr):\n",
    "#     loss = nn.CrossEntropyLoss()\n",
    "#     updater = torch.optim.SGD(net.parameters(), lr)\n",
    "#     for epoch in range(epoch):\n",
    "#         res = train_one_step(net, train_iter, loss, updater)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "num_steps =  35\n",
    "num_hiddens = 512\n",
    "lr = 1 \n",
    "\n",
    "print(\"hello world\")\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "\n",
    "test_rnn = RNNModelScratch(batch_size, len(vocab), num_hiddens, d2l.try_gpu())\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "# updater = lambda batch_size: d2l.sgd(test_rnn.params, lr, batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08318f74c127eea3b6abca5ce71e6f6674e1017b643dabd11b87c6f6a0087455"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
